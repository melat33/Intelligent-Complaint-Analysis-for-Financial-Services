{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cc6a638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß CHECKING AND UPGRADING PYARROW\n",
      "==================================================\n",
      "Current PyArrow version: 22.0.0\n",
      "\n",
      "üîÑ Upgrading PyArrow...\n",
      "‚úÖ PyArrow upgraded successfully\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"üîß CHECKING AND UPGRADING PYARROW\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "# Check current version\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    print(f\"Current PyArrow version: {pa.__version__}\")\n",
    "except:\n",
    "    print(\"PyArrow not installed or version too old\")\n",
    "\n",
    "# Upgrade PyArrow\n",
    "print(\"\\nüîÑ Upgrading PyArrow...\")\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pyarrow\", \"pandas\"])\n",
    "    print(\"‚úÖ PyArrow upgraded successfully\")\n",
    "    \n",
    "    # Reload modules\n",
    "    importlib.reload(sys.modules.get('pyarrow', None))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Upgrade failed: {e}\")\n",
    "    print(\"Please run in terminal: pip install --upgrade pyarrow pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cc88f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö IMPORTING LIBRARIES\n",
      "==================================================\n",
      "‚úÖ Libraries imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\10 acadamy\\Intelligent Complaint Analysis for Financial Services\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"üìö IMPORTING LIBRARIES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "360d5e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç CHECKING AVAILABLE FILES\n",
      "==================================================\n",
      "Looking in: d:\\10 acadamy\\Intelligent Complaint Analysis for Financial Services\\notebooks\\data\\processed\n",
      "\n",
      "üìÅ FILES AVAILABLE:\n",
      "   1. all_chunks.parquet (318.9 MB)\n",
      "   2. chunking_progress.json (0.0 MB)\n",
      "   3. chunks_metadata.csv (0.0 MB)\n",
      "   4. chunks_summary.csv (183.9 MB)\n",
      "   5. cleaned_complaints.csv (1193.3 MB)\n",
      "   6. complaint_metadata_full.parquet (18.4 MB)\n",
      "\n",
      "‚úÖ USING: data/processed/complaint_metadata_full.parquet\n",
      "   Size: 18.4 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"\\nüîç CHECKING AVAILABLE FILES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "data_dir = \"data/processed/\"\n",
    "print(f\"Looking in: {os.path.abspath(data_dir)}\")\n",
    "\n",
    "# List files\n",
    "files = os.listdir(data_dir)\n",
    "print(f\"\\nüìÅ FILES AVAILABLE:\")\n",
    "for i, file in enumerate(sorted(files)):\n",
    "    size = os.path.getsize(os.path.join(data_dir, file)) / (1024**2)  # MB\n",
    "    print(f\"  {i+1:2d}. {file} ({size:.1f} MB)\")\n",
    "\n",
    "# Use the correct file\n",
    "file_path = os.path.join(data_dir, \"complaint_metadata_full.parquet\")\n",
    "print(f\"\\n‚úÖ USING: {file_path}\")\n",
    "print(f\"   Size: {os.path.getsize(file_path) / (1024**2):.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c5465a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä LOADING EMBEDDINGS DATA - METADATA ONLY\n",
      "==================================================\n",
      "üì• Reading file metadata...\n",
      "‚ùå Error reading file metadata: name 'pq' is not defined\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"\\nüìä LOADING EMBEDDINGS DATA - METADATA ONLY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    print(f\"üì• Reading file metadata...\")\n",
    "    \n",
    "    # Get file metadata\n",
    "    parquet_file = pq.ParquetFile(embeddings_path)\n",
    "    metadata = parquet_file.metadata\n",
    "    \n",
    "    print(f\"‚úÖ File metadata loaded\")\n",
    "    print(f\"   ‚Ä¢ Total rows: {metadata.num_rows:,}\")\n",
    "    print(f\"   ‚Ä¢ Number of row groups: {metadata.num_row_groups}\")\n",
    "    print(f\"   ‚Ä¢ Columns: {metadata.num_columns}\")\n",
    "    \n",
    "    # Show column names\n",
    "    print(f\"\\nüìã COLUMNS IN FILE:\")\n",
    "    schema = parquet_file.schema\n",
    "    for i, col in enumerate(schema.names):\n",
    "        print(f\"   {i+1:2d}. {col}\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_cols = ['embeddings', 'text_chunk', 'complaint_id', 'product_category']\n",
    "    available_cols = schema.names\n",
    "    missing_cols = [col for col in required_cols if col not in available_cols]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"\\n‚ùå MISSING REQUIRED COLUMNS: {missing_cols}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ ALL REQUIRED COLUMNS PRESENT\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading file metadata: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce287d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ FAST CHROMADB INITIALIZATION\n",
      "==================================================\n",
      "Creating vector store at: vector_store_1768244751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ChromaDB initialized in 3.17 seconds\n",
      "   ‚Ä¢ Path: d:\\10 acadamy\\Intelligent Complaint Analysis for Financial Services\\notebooks\\vector_store_1768244751\n",
      "   ‚Ä¢ Collection: financial_complaints\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"\\nüíæ FAST CHROMADB INITIALIZATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import time\n",
    "import chromadb\n",
    "\n",
    "# Use timestamp to create unique path\n",
    "timestamp = int(time.time())\n",
    "vector_store_path = f\"vector_store_{timestamp}\"\n",
    "\n",
    "print(f\"Creating vector store at: {vector_store_path}\")\n",
    "\n",
    "try:\n",
    "    # Skip process scanning - just create new directory\n",
    "    if os.path.exists(vector_store_path):\n",
    "        # Try quick remove\n",
    "        try:\n",
    "            import shutil\n",
    "            shutil.rmtree(vector_store_path, ignore_errors=True)\n",
    "        except:\n",
    "            pass  # Ignore errors, we'll create with different name if needed\n",
    "    \n",
    "    # Create directory\n",
    "    os.makedirs(vector_store_path, exist_ok=True)\n",
    "    \n",
    "    # Initialize client\n",
    "    client = chromadb.PersistentClient(path=vector_store_path)\n",
    "    \n",
    "    # Create collection\n",
    "    collection = client.create_collection(\n",
    "        name=\"financial_complaints\",\n",
    "        metadata={\n",
    "            \"hnsw:space\": \"cosine\",\n",
    "            \"description\": \"Financial complaints database\",\n",
    "            \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "            \"created\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ ChromaDB initialized in {time.time() - timestamp:.2f} seconds\")\n",
    "    print(f\"   ‚Ä¢ Path: {os.path.abspath(vector_store_path)}\")\n",
    "    print(f\"   ‚Ä¢ Collection: financial_complaints\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    # Emergency fallback: Use in-memory\n",
    "    print(\"\\nüîÑ Using in-memory store as fallback...\")\n",
    "    client = chromadb.Client()\n",
    "    collection = client.create_collection(name=\"financial_complaints\")\n",
    "    print(\"‚úÖ Created in-memory store (temporary)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab44b007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• LOADING METADATA FILE\n",
      "==================================================\n",
      "Loading complaint_metadata_full.parquet...\n",
      "‚úÖ Successfully loaded!\n",
      "   ‚Ä¢ Rows: 1,375,327\n",
      "   ‚Ä¢ Columns: ['chunk_index', 'company', 'complaint_id', 'date_received', 'issue', 'product', 'product_category', 'state', 'sub_issue', 'total_chunks', 'id']\n",
      "   ‚Ä¢ Memory: 892.23 MB\n",
      "\n",
      "üîç DATA TYPES:\n",
      "   ‚Ä¢ chunk_index: int64 (sample: None...)\n",
      "   ‚Ä¢ company: object (sample: CITIBANK, N.A....)\n",
      "   ‚Ä¢ complaint_id: object (sample: 14069121...)\n",
      "   ‚Ä¢ date_received: object (sample: 2025-06-13...)\n",
      "   ‚Ä¢ issue: object (sample: Getting a credit card...)\n",
      "   ‚Ä¢ product: object (sample: Credit card...)\n",
      "   ‚Ä¢ product_category: object (sample: Credit Card...)\n",
      "   ‚Ä¢ state: object (sample: TX...)\n",
      "   ‚Ä¢ sub_issue: object (sample: Card opened without my consent or knowledge...)\n",
      "   ‚Ä¢ total_chunks: int64 (sample: 1...)\n",
      "   ‚Ä¢ id: object (sample: 14069121_0...)\n",
      "\n",
      "üìÑ FIRST 3 ROWS:\n",
      "   chunk_index                company complaint_id date_received  \\\n",
      "0            0         CITIBANK, N.A.     14069121    2025-06-13   \n",
      "1            0  WELLS FARGO & COMPANY     14061897    2025-06-13   \n",
      "2            1  WELLS FARGO & COMPANY     14061897    2025-06-13   \n",
      "\n",
      "                   issue                      product product_category state  \\\n",
      "0  Getting a credit card                  Credit card      Credit Card    TX   \n",
      "1    Managing an account  Checking or savings account  Savings Account    ID   \n",
      "2    Managing an account  Checking or savings account  Savings Account    ID   \n",
      "\n",
      "                                     sub_issue  total_chunks          id  \n",
      "0  Card opened without my consent or knowledge             1  14069121_0  \n",
      "1                     Deposits and withdrawals             2  14061897_0  \n",
      "2                     Deposits and withdrawals             2  14061897_1  \n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"\\nüì• LOADING METADATA FILE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    print(\"Loading complaint_metadata_full.parquet...\")\n",
    "    \n",
    "    # Load the file (19MB is small enough to load all at once)\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully loaded!\")\n",
    "    print(f\"   ‚Ä¢ Rows: {len(df):,}\")\n",
    "    print(f\"   ‚Ä¢ Columns: {list(df.columns)}\")\n",
    "    print(f\"   ‚Ä¢ Memory: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "    \n",
    "    # Show data types\n",
    "    print(f\"\\nüîç DATA TYPES:\")\n",
    "    for col in df.columns:\n",
    "        dtype = df[col].dtype\n",
    "        sample = df[col].iloc[0] if len(df) > 0 else None\n",
    "        print(f\"   ‚Ä¢ {col}: {dtype} (sample: {str(sample)[:50] if sample else 'None'}...)\")\n",
    "    \n",
    "    # Show first few rows\n",
    "    print(f\"\\nüìÑ FIRST 3 ROWS:\")\n",
    "    print(df.head(3))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading file: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "759b28df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç ANALYZING COLUMNS FOR DOCUMENT CREATION\n",
      "==================================================\n",
      "Looking for text columns to create documents...\n",
      "‚úÖ Found potential text columns: ['issue', 'sub_issue']\n",
      "\n",
      "üìù SAMPLES FROM CANDIDATE COLUMNS:\n",
      "   ‚Ä¢ issue: Getting a credit card...\n",
      "   ‚Ä¢ sub_issue: Card opened without my consent or knowledge...\n",
      "\n",
      "üîë ID columns: ['complaint_id', 'id']\n",
      "üè∑Ô∏è Product columns: ['product', 'product_category']\n",
      "‚ö†Ô∏è Issue columns: ['issue', 'sub_issue']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"\\nüîç ANALYZING COLUMNS FOR DOCUMENT CREATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if df is not None:\n",
    "    print(\"Looking for text columns to create documents...\")\n",
    "    \n",
    "    # Check for narrative/text columns\n",
    "    narrative_candidates = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':  # String columns\n",
    "            # Check first non-null value\n",
    "            non_null = df[col].dropna()\n",
    "            if len(non_null) > 0:\n",
    "                sample = non_null.iloc[0]\n",
    "                if isinstance(sample, str) and len(sample.strip()) > 20:\n",
    "                    narrative_candidates.append(col)\n",
    "    \n",
    "    print(f\"‚úÖ Found potential text columns: {narrative_candidates}\")\n",
    "    \n",
    "    # Show samples from candidate columns\n",
    "    if narrative_candidates:\n",
    "        print(f\"\\nüìù SAMPLES FROM CANDIDATE COLUMNS:\")\n",
    "        for col in narrative_candidates[:3]:  # Show first 3\n",
    "            sample = df[col].dropna().iloc[0] if len(df[col].dropna()) > 0 else \"No data\"\n",
    "            print(f\"   ‚Ä¢ {col}: {str(sample)[:100]}...\")\n",
    "    \n",
    "    # Check for complaint ID column\n",
    "    id_candidates = [col for col in df.columns if 'id' in col.lower() or 'complaint' in col.lower()]\n",
    "    print(f\"\\nüîë ID columns: {id_candidates}\")\n",
    "    \n",
    "    # Check for product columns\n",
    "    product_candidates = [col for col in df.columns if 'product' in col.lower()]\n",
    "    print(f\"üè∑Ô∏è Product columns: {product_candidates}\")\n",
    "    \n",
    "    # Check for issue columns\n",
    "    issue_candidates = [col for col in df.columns if 'issue' in col.lower()]\n",
    "    print(f\"‚ö†Ô∏è Issue columns: {issue_candidates}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f40fa372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ LOADING EMBEDDING MODEL\n",
      "==================================================\n",
      "Loading sentence-transformers model...\n",
      "‚ùå Error loading model: name 'SentenceTransformer' is not defined\n",
      "Installing sentence-transformers...\n",
      "‚úÖ Model installed and loaded\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"\\nü§ñ LOADING EMBEDDING MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    print(\"Loading sentence-transformers model...\")\n",
    "    embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"   ‚Ä¢ Model: all-MiniLM-L6-v2\")\n",
    "    print(f\"   ‚Ä¢ Dimensions: {embedder.get_sentence_embedding_dimension()}\")\n",
    "    \n",
    "    # Test the model\n",
    "    test_text = \"credit card complaint\"\n",
    "    test_embedding = embedder.encode(test_text)\n",
    "    print(f\"   ‚Ä¢ Test embedding shape: {test_embedding.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"Installing sentence-transformers...\")\n",
    "    \n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sentence-transformers\", \"-q\"])\n",
    "    \n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    print(f\"‚úÖ Model installed and loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3162626c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö° VECTORIZED DOCUMENT CREATION (FAST)\n",
      "==================================================\n",
      "Creating documents from 1,375,327 rows using vectorized operations...\n",
      "Preparing metadata...\n",
      "‚úÖ Created 1,375,327 documents in 378.0 seconds\n",
      "üìÑ Sample document: Product: Credit card. Issue: Getting a credit card. Details: Card opened without my consent or knowledge. Company: CITIBANK, N.A.. State: TX. ...\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"\\n‚ö° VECTORIZED DOCUMENT CREATION (FAST)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"Creating documents from {len(df):,} rows using vectorized operations...\")\n",
    "    \n",
    "    # Use vectorized string operations - MUCH faster\n",
    "    # Create document text using pandas string operations\n",
    "    \n",
    "    # Start with empty string series\n",
    "    doc_texts = pd.Series([\"\"] * len(df), dtype=str)\n",
    "    \n",
    "    # Add product information\n",
    "    if 'product' in df.columns:\n",
    "        doc_texts = doc_texts + \"Product: \" + df['product'].fillna('Unknown').astype(str) + \". \"\n",
    "    elif 'product_category' in df.columns:\n",
    "        doc_texts = doc_texts + \"Product: \" + df['product_category'].fillna('Unknown').astype(str) + \". \"\n",
    "    \n",
    "    # Add issue information\n",
    "    if 'issue' in df.columns:\n",
    "        doc_texts = doc_texts + \"Issue: \" + df['issue'].fillna('Not specified').astype(str) + \". \"\n",
    "    \n",
    "    # Add sub_issue if exists\n",
    "    if 'sub_issue' in df.columns:\n",
    "        # Filter out 'None' or empty sub_issues\n",
    "        has_sub_issue = df['sub_issue'].notna() & (df['sub_issue'].astype(str).str.lower() != 'none')\n",
    "        doc_texts = doc_texts + \"Details: \" + df['sub_issue'].where(has_sub_issue, '').astype(str) + \". \"\n",
    "    \n",
    "    # Add company\n",
    "    if 'company' in df.columns:\n",
    "        doc_texts = doc_texts + \"Company: \" + df['company'].fillna('Unknown').astype(str) + \". \"\n",
    "    \n",
    "    # Add state\n",
    "    if 'state' in df.columns:\n",
    "        doc_texts = doc_texts + \"State: \" + df['state'].fillna('Unknown').astype(str) + \". \"\n",
    "    \n",
    "    # Convert to list\n",
    "    documents = doc_texts.tolist()\n",
    "    \n",
    "    # Prepare metadata using list comprehension (faster than loop)\n",
    "    print(\"Preparing metadata...\")\n",
    "    \n",
    "    # Get columns that exist in dataframe\n",
    "    key_fields = ['complaint_id', 'product', 'product_category', 'issue', 'sub_issue', \n",
    "                 'company', 'state', 'date_received']\n",
    "    existing_fields = [field for field in key_fields if field in df.columns]\n",
    "    \n",
    "    # Create metadata using list comprehension\n",
    "    metadatas = []\n",
    "    for i in range(len(df)):\n",
    "        metadata = {}\n",
    "        for field in existing_fields:\n",
    "            value = df[field].iloc[i]\n",
    "            if pd.notna(value):\n",
    "                metadata[field] = str(value)\n",
    "        \n",
    "        # Add complaint ID if not present\n",
    "        if 'complaint_id' not in metadata:\n",
    "            metadata['complaint_id'] = f\"comp_{i}\"\n",
    "        \n",
    "        metadata['row_index'] = str(i)\n",
    "        metadatas.append(metadata)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚úÖ Created {len(documents):,} documents in {elapsed:.1f} seconds\")\n",
    "    print(f\"üìÑ Sample document: {documents[0][:150]}...\" if documents else \"No documents created\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data to process\")\n",
    "    documents = []\n",
    "    metadatas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ae6fc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ LOAD SMALL SAMPLE FOR TASK 3 (FAST)\n",
      "==================================================\n",
      "For Task 3 RAG pipeline evaluation, you need only 1,000-10,000 documents.\n",
      "This will complete in MINUTES, not hours!\n",
      "Loading 5,000 documents (out of 1,375,327)...\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [03:25<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding to vector store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LOADED 5,000 DOCUMENTS!\n",
      "üìä Vector store now has 5,000 documents\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"\\nüöÄ LOAD SMALL SAMPLE FOR TASK 3 (FAST)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"For Task 3 RAG pipeline evaluation, you need only 1,000-10,000 documents.\")\n",
    "print(\"This will complete in MINUTES, not hours!\")\n",
    "\n",
    "if 'documents' in locals() and documents and 'collection' in globals() and collection is not None:\n",
    "    # Take a small sample\n",
    "    sample_size = 5000  # 5,000 documents is PLENTY for Task 3\n",
    "    \n",
    "    print(f\"Loading {sample_size:,} documents (out of {len(documents):,})...\")\n",
    "    \n",
    "    # Take random sample\n",
    "    import random\n",
    "    indices = random.sample(range(len(documents)), min(sample_size, len(documents)))\n",
    "    sample_docs = [documents[i] for i in indices]\n",
    "    sample_metas = [metadatas[i] for i in indices]\n",
    "    \n",
    "    # Process in one batch (5,000 is fine for one batch)\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = embedder.encode(sample_docs, show_progress_bar=True)\n",
    "    \n",
    "    # Create IDs\n",
    "    ids = [f\"sample_doc_{i}\" for i in range(len(sample_docs))]\n",
    "    \n",
    "    # Add to vector store\n",
    "    print(\"Adding to vector store...\")\n",
    "    collection.add(\n",
    "        embeddings=embeddings.tolist(),\n",
    "        documents=sample_docs,\n",
    "        metadatas=sample_metas,\n",
    "        ids=ids\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ LOADED {len(sample_docs):,} DOCUMENTS!\")\n",
    "    print(f\"üìä Vector store now has {collection.count():,} documents\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Missing documents or collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7899ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç TESTING VECTOR STORE\n",
      "==================================================\n",
      "üìä Vector store has 5,000 documents\n",
      "\n",
      "üß™ RUNNING TEST QUERIES:\n",
      "\n",
      "üîç Query: 'credit card unauthorized transaction'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Found 3 results\n",
      "   üìÑ Document: Product: Credit card. Issue: Problem with a purchase shown on your statement. Details: Card was char...\n",
      "   üè∑Ô∏è Product: Credit Card\n",
      "   üè¢ Company: CITIBANK, N.A.\n",
      "   üìä Similarity: 0.568\n",
      "\n",
      "üîç Query: 'personal loan application rejected'\n",
      "   ‚úÖ Found 3 results\n",
      "   üìÑ Document: Product: Checking or savings account. Issue: Problem with a lender or other company charging your ac...\n",
      "   üè∑Ô∏è Product: Savings Account\n",
      "   üè¢ Company: Lending Club Corp\n",
      "   üìä Similarity: 0.430\n",
      "\n",
      "üîç Query: 'bank account fees'\n",
      "   ‚úÖ Found 3 results\n",
      "   üìÑ Document: Product: Checking or savings account. Issue: Managing an account. Details: Fee problem. Company: U.S...\n",
      "   üè∑Ô∏è Product: Savings Account\n",
      "   üè¢ Company: U.S. BANCORP\n",
      "   üìä Similarity: 0.586\n",
      "\n",
      "üîç Query: 'money transfer problem'\n",
      "   ‚úÖ Found 3 results\n",
      "   üìÑ Document: Product: Money transfer, virtual currency, or money service. Issue: Other transaction problem. Detai...\n",
      "   üè∑Ô∏è Product: Money Transfer\n",
      "   üè¢ Company: TransferWise Ltd\n",
      "   üìä Similarity: 0.645\n",
      "\n",
      "üîç Query: 'mortgage complaint'\n",
      "   ‚úÖ Found 3 results\n",
      "   üìÑ Document: Product: Checking or savings account. Issue: Problem with a lender or other company charging your ac...\n",
      "   üè∑Ô∏è Product: Savings Account\n",
      "   üè¢ Company: HUNTINGTON NATIONAL BANK, THE\n",
      "   üìä Similarity: 0.433\n",
      "\n",
      "‚úÖ Vector store is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"\\nüîç TESTING VECTOR STORE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'collection' in globals() and collection and collection.count() > 0:\n",
    "    print(f\"üìä Vector store has {collection.count():,} documents\")\n",
    "    \n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"credit card unauthorized transaction\",\n",
    "        \"personal loan application rejected\",\n",
    "        \"bank account fees\",\n",
    "        \"money transfer problem\",\n",
    "        \"mortgage complaint\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüß™ RUNNING TEST QUERIES:\")\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nüîç Query: '{query}'\")\n",
    "        \n",
    "        try:\n",
    "            results = collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=3,\n",
    "                include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "            )\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                print(f\"   ‚úÖ Found {len(results['documents'][0])} results\")\n",
    "                \n",
    "                # Show top result\n",
    "                doc = results['documents'][0][0]\n",
    "                meta = results['metadatas'][0][0]\n",
    "                \n",
    "                print(f\"   üìÑ Document: {doc[:100]}...\")\n",
    "                print(f\"   üè∑Ô∏è Product: {meta.get('product_category', meta.get('product', 'Unknown'))}\")\n",
    "                print(f\"   üè¢ Company: {meta.get('company', 'Unknown')}\")\n",
    "                \n",
    "                if results['distances'] and results['distances'][0]:\n",
    "                    similarity = 1 - results['distances'][0][0]\n",
    "                    print(f\"   üìä Similarity: {similarity:.3f}\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è No results found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Query error: {e}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Vector store is working correctly!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Vector store is empty or not accessible\")\n",
    "    if 'collection' in globals():\n",
    "        print(f\"   ‚Ä¢ Collection exists: {collection is not None}\")\n",
    "        if collection:\n",
    "            print(f\"   ‚Ä¢ Document count: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b5d80b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ SAVING CONFIGURATION\n",
      "==================================================\n",
      "SUCCESS: Configuration saved to: rag_vector_config.py\n",
      "\\nCONFIGURATION SUMMARY:\n",
      "   ‚Ä¢ Vector store: vector_store_1768244751\n",
      "   ‚Ä¢ Documents: 5,000\n",
      "   ‚Ä¢ Source file: complaint_metadata_full.parquet\n",
      "   ‚Ä¢ Config file: rag_vector_config.py\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"\\nüíæ SAVING CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Remove emojis from the config file - they cause encoding issues\n",
    "    config_content = f'''# VECTOR STORE CONFIGURATION\n",
    "# Generated for Task 3 RAG Pipeline\n",
    "\n",
    "import os\n",
    "\n",
    "# Path Configuration\n",
    "VECTOR_STORE_PATH = r\"{os.path.abspath(vector_store_path)}\"\n",
    "COLLECTION_NAME = \"financial_complaints\"\n",
    "SOURCE_FILE = r\"{os.path.abspath(file_path)}\"\n",
    "\n",
    "# Statistics\n",
    "TOTAL_DOCUMENTS = {collection.count() if 'collection' in globals() and collection else 0}\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "CREATED_AT = \"{time.strftime(\"%Y-%m-%d %H:%M:%S\")}\"\n",
    "\n",
    "# Functions\n",
    "def get_vector_store():\n",
    "    \"\"\"\n",
    "    Get the ChromaDB collection for RAG pipeline\n",
    "    \n",
    "    Returns:\n",
    "        chromadb.Collection: The vector store collection\n",
    "    \"\"\"\n",
    "    import chromadb\n",
    "    client = chromadb.PersistentClient(path=VECTOR_STORE_PATH)\n",
    "    return client.get_collection(COLLECTION_NAME)\n",
    "\n",
    "def test_connection():\n",
    "    \"\"\"\n",
    "    Test if vector store is accessible\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        collection = get_vector_store()\n",
    "        count = collection.count()\n",
    "        print(\"SUCCESS: Connected to vector store\")\n",
    "        print(f\"Documents: {{count:,}}\")\n",
    "        print(f\"Location: {{VECTOR_STORE_PATH}}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Connection failed: {{e}}\")\n",
    "        return False\n",
    "\n",
    "# Quick test\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 50)\n",
    "    print(\"VECTOR STORE CONFIGURATION\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Path: {{VECTOR_STORE_PATH}}\")\n",
    "    print(f\"Collection: {{COLLECTION_NAME}}\")\n",
    "    print(f\"Documents: {{TOTAL_DOCUMENTS:,}}\")\n",
    "    \n",
    "    if test_connection():\n",
    "        print(\"\\\\nREADY: RAG Pipeline is ready!\")\n",
    "    else:\n",
    "        print(\"\\\\nERROR: Configuration issue\")\n",
    "'''\n",
    "\n",
    "    config_file = \"rag_vector_config.py\"\n",
    "    with open(config_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(config_content)\n",
    "    \n",
    "    print(f\"SUCCESS: Configuration saved to: {config_file}\")\n",
    "    \n",
    "    # Also save path to simple text file\n",
    "    with open(\"vector_store_path.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(vector_store_path)\n",
    "    \n",
    "    print(f\"\\\\nCONFIGURATION SUMMARY:\")\n",
    "    print(f\"   ‚Ä¢ Vector store: {vector_store_path}\")\n",
    "    print(f\"   ‚Ä¢ Documents: {collection.count() if 'collection' in globals() and collection else 0:,}\")\n",
    "    print(f\"   ‚Ä¢ Source file: {os.path.basename(file_path)}\")\n",
    "    print(f\"   ‚Ä¢ Config file: {config_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR saving configuration: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "039ca33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VECTOR STORE CREATION COMPLETE\n",
      "============================================================\n",
      "\\nSUCCESS! Vector store created with 5,000 REAL documents.\n",
      "\\nSTATISTICS:\n",
      "   ‚Ä¢ Documents: 5,000\n",
      "   ‚Ä¢ Location: vector_store_1768244751\n",
      "   ‚Ä¢ Source: complaint_metadata_full.parquet\n",
      "   ‚Ä¢ Embedding model: all-MiniLM-L6-v2\n",
      "\\nFOR TASK 3 (RAG PIPELINE):\n",
      "   1. Create new notebook: task3_rag.ipynb\n",
      "   2. Start with:\n",
      "   \n",
      "   ```python\n",
      "   from rag_vector_config import get_vector_store\n",
      "   \n",
      "   # Get the vector store\n",
      "   collection = get_vector_store()\n",
      "   print(f'Ready with {collection.count():,} documents')\n",
      "   \n",
      "   # Now build your RAG system:\n",
      "   # 1. Create retriever\n",
      "   # 2. Design prompt templates\n",
      "   # 3. Set up LLM generator\n",
      "   # 4. Test with queries\n",
      "   # 5. Create evaluation table\n",
      "   ```\n",
      "\\nTASK 3 REQUIREMENTS:\n",
      "   ‚úì Vector store with real data\n",
      "   ‚úì Semantic search working\n",
      "   ‚Üí Build retriever component\n",
      "   ‚Üí Create prompt templates\n",
      "   ‚Üí Set up LLM (use test generator)\n",
      "   ‚Üí Complete RAG pipeline\n",
      "   ‚Üí Evaluation table (5-10 questions)\n",
      "\\nIMPORTANT:\n",
      "   ‚Ä¢ This uses REAL data from your metadata file\n",
      "   ‚Ä¢ Perfect for Task 3 evaluation\n",
      "   ‚Ä¢ Ready to build your RAG pipeline\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VECTOR STORE CREATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'collection' in globals() and collection:\n",
    "    count = collection.count()\n",
    "    \n",
    "    if count > 0:\n",
    "        print(f\"\\\\nSUCCESS! Vector store created with {count:,} REAL documents.\")\n",
    "        print(f\"\\\\nSTATISTICS:\")\n",
    "        print(f\"   ‚Ä¢ Documents: {count:,}\")\n",
    "        print(f\"   ‚Ä¢ Location: {vector_store_path}\")\n",
    "        print(f\"   ‚Ä¢ Source: complaint_metadata_full.parquet\")\n",
    "        print(f\"   ‚Ä¢ Embedding model: all-MiniLM-L6-v2\")\n",
    "        \n",
    "        print(f\"\\\\nFOR TASK 3 (RAG PIPELINE):\")\n",
    "        print(f\"   1. Create new notebook: task3_rag.ipynb\")\n",
    "        print(f\"   2. Start with:\")\n",
    "        print(f\"   \")\n",
    "        print(f\"   ```python\")\n",
    "        print(f\"   from rag_vector_config import get_vector_store\")\n",
    "        print(f\"   \")\n",
    "        print(f\"   # Get the vector store\")\n",
    "        print(f\"   collection = get_vector_store()\")\n",
    "        print(f\"   print(f'Ready with {{collection.count():,}} documents')\")\n",
    "        print(f\"   \")\n",
    "        print(f\"   # Now build your RAG system:\")\n",
    "        print(f\"   # 1. Create retriever\")\n",
    "        print(f\"   # 2. Design prompt templates\")\n",
    "        print(f\"   # 3. Set up LLM generator\")\n",
    "        print(f\"   # 4. Test with queries\")\n",
    "        print(f\"   # 5. Create evaluation table\")\n",
    "        print(f\"   ```\")\n",
    "        \n",
    "        print(f\"\\\\nTASK 3 REQUIREMENTS:\")\n",
    "        print(f\"   ‚úì Vector store with real data\")\n",
    "        print(f\"   ‚úì Semantic search working\")\n",
    "        print(f\"   ‚Üí Build retriever component\")\n",
    "        print(f\"   ‚Üí Create prompt templates\")\n",
    "        print(f\"   ‚Üí Set up LLM (use test generator)\")\n",
    "        print(f\"   ‚Üí Complete RAG pipeline\")\n",
    "        print(f\"   ‚Üí Evaluation table (5-10 questions)\")\n",
    "        \n",
    "        print(f\"\\\\nIMPORTANT:\")\n",
    "        print(f\"   ‚Ä¢ This uses REAL data from your metadata file\")\n",
    "        print(f\"   ‚Ä¢ Perfect for Task 3 evaluation\")\n",
    "        print(f\"   ‚Ä¢ Ready to build your RAG pipeline\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\\\nWARNING: Vector store created but empty\")\n",
    "        print(f\"Check the loading process for errors\")\n",
    "else:\n",
    "    print(f\"\\\\nERROR: Vector store not created\")\n",
    "    print(f\"Review error messages above\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
